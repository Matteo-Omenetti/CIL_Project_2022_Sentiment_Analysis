{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing_Matteo.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"1jVrOExYS96j"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"5s03gfomSz1n","executionInfo":{"status":"ok","timestamp":1658779144709,"user_tz":-120,"elapsed":3910,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"outputs":[],"source":["%%capture\n","!pip install nltk"]},{"cell_type":"code","source":["%%capture\n","import numpy as np\n","import sklearn\n","import pandas as pd\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"298Gg8IFS9gV","executionInfo":{"status":"ok","timestamp":1658779146652,"user_tz":-120,"elapsed":1951,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLIkk5YGTSzE","outputId":"88b9680e-e515-4e1d-fa57-6ecec7d8db67","executionInfo":{"status":"ok","timestamp":1658779180862,"user_tz":-120,"elapsed":34214,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# constants and global variables\n","DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'\n","\n","# if set to True the preprocessing for the bert model will be done, otherswise \n","# the preprocessig for w2v and Tf-idf will be performed\n","is_bert_preprocessing_enabled = True\n","use_additional_dataset = False"],"metadata":{"id":"OrOVITskTH7V","executionInfo":{"status":"ok","timestamp":1658779180863,"user_tz":-120,"elapsed":10,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Read Data"],"metadata":{"id":"0ldj0wg4TUAj"}},{"cell_type":"code","source":["tweets = []\n","labels = []\n","\n","if use_additional_dataset:\n","  # read the additional dataset\n","  df = pd.read_csv(DATA_PATH.format('additional_dataset.csv'), delimiter=',', encoding = 'latin',header=None)\n","  # df = df.drop(df.columns[[1, 2, 3, 4]], axis=1)\n","  nRow, nCol = df.shape\n","  print(f'There are {nRow} rows and {nCol} columns')\n","\n","\n","  # the additional tweets\n","  tweets = df[5].tolist()\n","  # substitute links with <url>\n","  import re\n","  tweets = list(map(lambda tweet : re.sub(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", \"<url>\", tweet), tweets))\n","  # substitute usernames with <user>\n","  tweets = list(map(lambda tweet : re.sub(\"@\\w+\", \"<user>\", tweet), tweets))\n","\n","  # the additional labels\n","  labels = df[0].tolist()\n","\n","  for i in range(len(labels)):\n","    if labels[i] == 4:\n","      labels[i] = 1\n","\n","  for i in range(30):\n","    print(tweets[i])\n","    print(labels[i])\n","  df.head()"],"metadata":{"id":"pDYgxdFpG7N0","executionInfo":{"status":"ok","timestamp":1658779180863,"user_tz":-120,"elapsed":7,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def load_tweets(filename, label):\n","    with open(filename, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            tweets.append(line.rstrip())\n","            labels.append(label)\n","\n","# load training tweets   \n","load_tweets(DATA_PATH.format('train_neg_full.txt'), 0)\n","load_tweets(DATA_PATH.format('train_pos_full.txt'), 1)\n","# Convert to NumPy array to facilitate indexing\n","print(f'{len(tweets)} training/dev tweets loaded')\n","tweets = np.array(tweets)\n","labels = np.array(labels)\n","\n","# load the test file\n","f = open(DATA_PATH.format('test_data.txt'), 'r', encoding='utf-8')\n","X_test = []\n","for line in f:\n","  X_test.append(\",\".join(line.split(',')[1:]).strip())\n","X_test = np.array(X_test)\n","print(f'{len(X_test)} test tweets loaded')\n","\n","\n","for i in range(10):\n","    print(tweets[i])\n","    print(labels[i])\n","\n","for i in range(10):\n","  print(X_test[i])\n","\n","print(len(tweets))"],"metadata":{"id":"Pg1VSXtvTQo1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea23b792-0b86-4892-c70c-63b3e5a17b79","executionInfo":{"status":"ok","timestamp":1658779187921,"user_tz":-120,"elapsed":7064,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2500000 training/dev tweets loaded\n","10000 test tweets loaded\n","vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n","0\n","glad i dot have taks tomorrow ! ! #thankful #startho\n","0\n","1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n","0\n","<user> i could actually kill that girl i'm so sorry ! ! !\n","0\n","<user> <user> <user> i find that very hard to believe im afraid\n","0\n","wish i could be out all night tonight ! <user>\n","0\n","<user> i got kicked out the wgm\n","0\n","rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n","0\n","why is she so perfect <url>\n","0\n","<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n","0\n","sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n","<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n","i cant stay away from bug thats my baby\n","<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n","whenever i fall asleep watching the tv , i always wake up with a headache\n","<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n","its whatever . in a terrible mood ( (\n","yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\n","my friend <user> text me to check up on me last night .\n","<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n","2500000\n"]}]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"0-b6wdk3TbVl"}},{"cell_type":"code","source":["# preprocess for bert\n","if is_bert_preprocessing_enabled:\n","\n","  # Lowercase sentence\n","  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n","  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n","\n","  # # remove hashtags\n","  # import re\n","  # tweets = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), tweets))\n","  # X_test = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), X_test))\n","\n","  # remove duplicates from the training data (decreases accuracy)\n","  # print(len(tweets))\n","  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n","  # tweets = tweets_df[\"tweets\"].to_numpy()\n","  # labels = tweets_df[\"labels\"].to_numpy()\n","  # print(len(tweets))\n","\n","  # split each tweet into separate words\n","  # tweets = list(map(lambda tweet : tweet.split(), tweets))\n","  # X_test = list(map(lambda tweet : tweet.split(), X_test))\n","  \n","  # remove user, url and other commond words (decreases accuracy)\n","  # forbidden_words = [\"<url>\", \"<user>\"]\n","  # tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n","  # X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n","\n","  # from nltk.tokenize import RegexpTokenizer\n","  # # Tokenize sentence\n","  # tokenizer = RegexpTokenizer(r'\\w+')\n","  # tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n","  # X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n","\n","  # Remove stopwords (decrease accuracy)\n","  # from nltk.corpus import stopwords\n","  # stopwords_set = stopwords.words('english')\n","  # tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n","  # X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n","\n","  # remove numbers (decreases accuracy)\n","  # import re\n","  # tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n","  # X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n","\n","  # # join to back the tweets into a phrase\n","  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n","  # X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n","\n","  # # remove duplicates from the training data\n","  print(len(tweets))\n","  tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets', 'labels'], keep='last')\n","  tweets = tweets_df[\"tweets\"].to_numpy()\n","  labels = tweets_df[\"labels\"].to_numpy()\n","  print(len(tweets))\n"],"metadata":{"id":"CfBSuzHlLR9r","executionInfo":{"status":"ok","timestamp":1658779194270,"user_tz":-120,"elapsed":6353,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbb25351-ad08-4954-e102-02e073ed4027"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2500000\n","2270482\n"]}]},{"cell_type":"code","source":["# preprocess for non bert\n","if not is_bert_preprocessing_enabled:\n","  # Lowercase sentence\n","  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n","  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n","\n","  from nltk.tokenize import RegexpTokenizer\n","  # Tokenize sentence\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n","  X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n","\n","  # remove hashtags\n","  tweets = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], tweets))\n","  X_test = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], X_test))\n","\n","  from nltk.corpus import stopwords\n","  # Remove stopwords\n","  stopwords_set = stopwords.words('english')\n","  tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n","  X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n","\n","  from nltk.stem import WordNetLemmatizer\n","  # Lemmatize\n","  lemmatizer = WordNetLemmatizer()\n","  tweets = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], tweets))\n","  X_test = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], X_test))\n","\n","  # remove numbers\n","  import re\n","  tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n","  X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n","\n","  # remove user, and url\n","  forbidden_words = [\"url\", \"user\"]\n","  tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n","  X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n","\n","  # # remove duplicates from the training data\n","  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n","  # print(len(tweets))\n","  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n","  # tweets = tweets_df[\"tweets\"].to_numpy()\n","  # labels = tweets_df[\"labels\"].to_numpy()\n","  # print(len(tweets))\n","\n","  # join back the tweets into a phrase\n","  X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n","  tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n"],"metadata":{"id":"NMpcCsG3TarA","executionInfo":{"status":"ok","timestamp":1658779194271,"user_tz":-120,"elapsed":8,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["for i in range(20):\n","  print(tweets[i])\n","\n","print(\"\\n\\n\\n\")\n","\n","for i in range(20):\n","  print(X_test[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMoLGO9XT17_","outputId":"137ff3ee-1725-4b5c-87e9-06ab864fb15b","executionInfo":{"status":"ok","timestamp":1658779194271,"user_tz":-120,"elapsed":5,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n","glad i dot have taks tomorrow ! ! #thankful #startho\n","1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n","<user> i could actually kill that girl i'm so sorry ! ! !\n","<user> <user> <user> i find that very hard to believe im afraid\n","wish i could be out all night tonight ! <user>\n","<user> i got kicked out the wgm\n","rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n","why is she so perfect <url>\n","<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n","introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n","<user> i'm white . #aw\n","<user> dan i love and miss you ! don't be sad #wheresthegeneral\n","so many wonderful building in dc but still miss you <user>\n","<user> it's annoying because i secretly find it so good ...\n","the post-boom in spanish american fiction ( suny series in latin american and iberian thought and culture what ... <url>\n","layers of the heart ( paperback this journey was inspired by a recent robbery that took place in the united sta ... <url>\n","guess who texted me again and wants us back ( ( ( <user> <user> <user>\n","<user> # 2 farrow ( v a litter offence ; wilful dispersal of piglets ~ continued\n","99000 people at barca's ground tonight are gonna see chelsea get absolutly destroyed ! the other 100million will have to watch on telly\n","\n","\n","\n","\n","sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n","<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n","i cant stay away from bug thats my baby\n","<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n","whenever i fall asleep watching the tv , i always wake up with a headache\n","<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n","its whatever . in a terrible mood ( (\n","yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\n","my friend <user> text me to check up on me last night .\n","<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n","watch some of y'all dumb asses get lock up today #happy420\n","obsessed with #phasell <user> you killed it ! ! ! best album ever love yew roycee ! ! : * rt me\n","<user> robert de niro is not gay .. but with a name like lewy , i'd understand if you were hahahahha no . i am sherlock\n","<user> canada have to do it in grade 12 . but since we don't have grade 12 here , we do it in 11th . it sucks\n","<user> please say hi to denmark ! that would be amazing ! ( <user> live on <url>\n","finally am home now\n","3x3 custom picture frame / poster frame 1.2 \" wide complete gold frame ( 2380763 9gd this frame is manufactured i ... <url>\n","s / o to my new followers . mention me for a followback boo\n","<user> yep , looks like the best team will stay up . proper on form . see you in a bit\n","nhl's bettman : suspension criticism ' gamesmanship ' ( the associated press new york ( ap ) nhl ... <url> #predators #nhl\n"]}]},{"cell_type":"code","source":["# shuffle the training data\n","from sklearn.utils import shuffle\n","\n","X_train, y_train = shuffle(tweets, labels, random_state=84)"],"metadata":{"id":"4YqG3R8WT4bM","executionInfo":{"status":"ok","timestamp":1658779194725,"user_tz":-120,"elapsed":458,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# save X data to a file\n","def save_X_data(file_name, X):\n","  f = open(DATA_PATH.format(file_name), \"w\")\n","  for element in X:\n","      # f.write(\" \".join(element) + \"\\n\")\n","      f.write(element + \"\\n\")\n","  f.close()\n","\n","save_X_data(\"X_train_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_train_processed.txt\", X_train)\n","save_X_data(\"X_test_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_test_processed.txt\", X_test)"],"metadata":{"id":"CKAb04McUEh5","executionInfo":{"status":"ok","timestamp":1658779200003,"user_tz":-120,"elapsed":5281,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# save labels to file\n","f = open(DATA_PATH.format(\"y_train.txt\"), \"w\")\n","for label in y_train:\n","  f.write(str(label) + \"\\n\")\n","f.close()"],"metadata":{"id":"1v4OYiq0Wvda","executionInfo":{"status":"ok","timestamp":1658779203380,"user_tz":-120,"elapsed":3382,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLGfqcvEHhRj"},"source":["# INITIAL EXPLORATION"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YGXIVtwF1C7","outputId":"e60a41fa-c5a5-44a6-ec03-3ef0a5cb895d","executionInfo":{"status":"ok","timestamp":1658779203381,"user_tz":-120,"elapsed":6,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["In our dataset there are 1142838 negative tweets\n","In our dataset there are 1127644 positive tweets\n"]}],"source":["# get number of positive and negative tweets\n","print(f\"In our dataset there are {(labels == 0).sum()} negative tweets\")\n","print(f\"In our dataset there are {(labels == 1).sum()} positive tweets\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivQ_UyYXD1zi","outputId":"3988409e-6102-4ce8-a0bf-4008a54dd6af","executionInfo":{"status":"ok","timestamp":1658779205326,"user_tz":-120,"elapsed":1948,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The average length of tweets with a negative sentiment is: 83.0450676298828\n","The average length of tweets with a positive sentiment is: 68.3103497203018\n"]}],"source":["# get the average length of positive and negative examples\n","def get_average_length(target_label):\n","  len_tweets = []\n","  for tweet, label in zip(tweets, labels):\n","    if label == target_label:\n","      len_tweets.append(len(tweet))\n","  \n","  return np.array(len_tweets).mean()\n","\n","# NOTE: postive tweets seem to be longer on average than negative tweets.\n","print(f\"The average length of tweets with a negative sentiment is: {get_average_length(0)}\")\n","print(f\"The average length of tweets with a positive sentiment is: {get_average_length(1)}\")"]},{"cell_type":"code","source":["# get most frequent words in positive and negative examples\n","def count_words(target_label):\n","  words_occurrences = {}\n","  for tweet, label in zip(tweets, labels):\n","    if label == target_label:\n","      for word in tweet:\n","        words_occurrences[word] = words_occurrences.get(word, 0) + 1\n","\n","  return words_occurrences\n","\n","\n","from collections import Counter\n","\n","tweets = list(map(lambda x : x.split(), tweets))\n","# negative tweets\n","print(\"NEGATIVE TWEETS:\")\n","negative = count_words(0)\n","print(dict(Counter(negative).most_common(100)))\n","# print uniques words in the negative tweets\n","print(len(negative))\n","\n","# positive tweets\n","print(\"\\n\\n\\n POSITIVE TWEETS:\")\n","positive = count_words(1)\n","print(dict(Counter(positive).most_common(100)))\n","# print uniques words in the positive tweets\n","print(len(positive))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTDyENY06On1","outputId":"ef22c376-3bc5-42ba-c66b-f00197aeb4d4","executionInfo":{"status":"ok","timestamp":1658779226630,"user_tz":-120,"elapsed":21308,"user":{"displayName":"Matteo Omenetti","userId":"02799931472704688143"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["NEGATIVE TWEETS:\n","{'<user>': 557268, 'i': 514794, '(': 461668, 'the': 384629, '...': 361101, '<url>': 359419, ',': 357807, '!': 356189, '.': 337791, 'to': 330581, 'a': 230752, 'and': 220914, 'of': 193821, 'my': 190918, 'you': 183583, 'is': 161676, 'in': 152527, 'me': 152130, '-': 136086, 'for': 133763, '?': 133155, '\"': 129374, 'this': 123870, 'it': 117566, ':': 109284, 'so': 97197, 'with': 95146, 'frame': 86014, 'on': 85768, 'that': 83819, 'but': 82429, \"i'm\": 81933, 'have': 79842, '/': 73215, 'be': 69675, ')': 68844, 'not': 68645, 'just': 63839, 'was': 63622, 'at': 55280, '..': 52972, 'rt': 52687, 'like': 51955, 'no': 51219, 'all': 49390, 'are': 48510, 'now': 47197, 'get': 46593, 'your': 46463, 'up': 46455, 'go': 45615, 'when': 45394, \"don't\": 44834, 'do': 41269, 'one': 40712, '&': 40295, 'want': 39987, 'know': 38845, '2': 38684, 'miss': 38611, 'from': 37613, 'u': 36790, 'out': 36603, 'paperback': 35618, 'really': 34951, 'too': 34408, \"can't\": 33935, 'what': 33443, 'we': 33327, 'why': 32878, \"'\": 31685, 'love': 30922, 'can': 29842, 'lol': 29610, 'please': 29295, 'back': 29196, \"it's\": 29102, 'its': 28939, 'how': 28697, 'will': 28577, 'see': 28173, 'wish': 27885, 'day': 27718, 'if': 27648, 'x': 27329, 'an': 27286, 'im': 27249, 'got': 27239, 'pack': 27103, 'black': 27076, 'going': 27007, 'time': 26794, 'about': 26768, 'need': 26450, 'by': 26395, 'complete': 26345, 'he': 25262, 'as': 25087, 'today': 24988, 'picture': 24485}\n","405152\n","\n","\n","\n"," POSITIVE TWEETS:\n","{'<user>': 951004, '!': 585557, 'i': 421402, 'you': 352388, '.': 346378, 'to': 315889, ',': 305792, 'the': 296989, 'a': 239676, 'and': 194546, 'my': 188734, '?': 176191, 'me': 165725, 'it': 145973, 'for': 139399, 'is': 119070, 'in': 117440, 'of': 108020, 'that': 102947, 'on': 102615, '\"': 99335, ')': 94942, 'be': 94392, '...': 91500, 'so': 89969, 'with': 87821, 'have': 83628, '<url>': 82896, 'love': 81234, \"i'm\": 80137, 'your': 78962, 'just': 77975, 'this': 74391, 'rt': 71379, 'but': 68218, 'good': 67922, 'lol': 64716, '..': 61604, 'are': 60796, 'like': 59823, 'all': 55567, 'u': 54856, 'get': 53565, 'was': 52937, 'at': 52769, 'we': 52020, 'know': 51466, 'can': 51082, 'follow': 50261, 'do': 49369, 'if': 48938, '&': 48623, 'thanks': 48351, 'not': 47463, 'up': 47280, 'day': 46608, 'will': 46417, 'one': 45301, 'what': 43967, 'haha': 42268, 'now': 40917, 'when': 40472, 'out': 39632, 'please': 39243, '<3': 39158, 'see': 38761, '-': 38366, 'go': 37983, 'too': 37906, \"it's\": 35673, \"don't\": 34563, ':': 33647, 'got': 33573, 'about': 33471, 'back': 32781, 'no': 32778, 'time': 32651, '(': 32450, 'how': 32068, 'from': 31271, 'today': 30829, \"'\": 30446, 'x': 29988, 'happy': 29617, 'its': 29597, 'think': 29348, 'well': 29236, 'thank': 28722, 'im': 28523, 'going': 27444, 'he': 27205, 'her': 26898, \"i'll\": 26407, '*': 26045, 'want': 25606, 'there': 25545, 'some': 25508, 'as': 25402, 'then': 25130, 'really': 25027}\n","297267\n"]}]}]}