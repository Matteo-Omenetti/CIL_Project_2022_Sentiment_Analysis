{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_Matteo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wLGfqcvEHhRj"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1jVrOExYS96j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s03gfomSz1n",
        "outputId": "74c7ff9d-4535-4c52-cacc-a4707268ee1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298Gg8IFS9gV",
        "outputId": "d3e611d8-5eec-4242-cddb-aedda19caef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLIkk5YGTSzE",
        "outputId": "9c75e7b1-629e-4869-9bbb-020a0e702fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and global variables\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'\n",
        "\n",
        "# if set to True the preprocessing for the bert model will be done, otherswise \n",
        "# the preprocessig for w2v and Tf-idf will be performed\n",
        "is_bert_preprocessing_enabled = True"
      ],
      "metadata": {
        "id": "OrOVITskTH7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "0ldj0wg4TUAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "\n",
        "def load_tweets(filename, label):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweets.append(line.rstrip())\n",
        "            labels.append(label)\n",
        "\n",
        "# load training tweets   \n",
        "load_tweets(DATA_PATH.format('train_neg_full.txt'), 0)\n",
        "load_tweets(DATA_PATH.format('train_pos_full.txt'), 1)\n",
        "# Convert to NumPy array to facilitate indexing\n",
        "print(f'{len(tweets)} training/dev tweets loaded')\n",
        "tweets = np.array(tweets)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# load the test file\n",
        "f = open(DATA_PATH.format('test_data.txt'), 'r', encoding='utf-8')\n",
        "X_test = []\n",
        "for line in f:\n",
        "  X_test.append(line.split(',')[1].rstrip())\n",
        "X_test = np.array(X_test)\n",
        "print(f'{len(X_test)} test tweets loaded')\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(tweets[i])\n",
        "    print(labels[i])\n",
        "\n",
        "for i in range(10):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "id": "Pg1VSXtvTQo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3373599-70ee-4bbc-f6b5-dba3a0c0f64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500000 training/dev tweets loaded\n",
            "10000 test tweets loaded\n",
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "0\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "0\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "0\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "0\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "0\n",
            "wish i could be out all night tonight ! <user>\n",
            "0\n",
            "<user> i got kicked out the wgm\n",
            "0\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "0\n",
            "why is she so perfect <url>\n",
            "0\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "0\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "0-b6wdk3TbVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for bert\n",
        "if is_bert_preprocessing_enabled:\n",
        "  pass\n",
        "  # # Lowercase sentence\n",
        "  # tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  # X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  # # remove hashtags\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), X_test))\n",
        "\n",
        "  # remove duplicates from the training data (decreases accuracy)\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # split each tweet into separate words\n",
        "  # tweets = list(map(lambda tweet : tweet.split(), tweets))\n",
        "  # X_test = list(map(lambda tweet : tweet.split(), X_test))\n",
        "  \n",
        "  # remove user, url and other commond words (decreases accuracy)\n",
        "  # forbidden_words = [\"<url>\", \"<user>\"]\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # from nltk.tokenize import RegexpTokenizer\n",
        "  # # Tokenize sentence\n",
        "  # tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # Remove stopwords (decrease accuracy)\n",
        "  # from nltk.corpus import stopwords\n",
        "  # stopwords_set = stopwords.words('english')\n",
        "  # tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  # X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  # remove numbers (decreases accuracy)\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # # join to back the tweets into a phrase\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "CfBSuzHlLR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for non bert\n",
        "if not is_bert_preprocessing_enabled:\n",
        "  # Lowercase sentence\n",
        "  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  from nltk.tokenize import RegexpTokenizer\n",
        "  # Tokenize sentence\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # remove hashtags\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], X_test))\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "  # Remove stopwords\n",
        "  stopwords_set = stopwords.words('english')\n",
        "  tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  # Lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweets = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], tweets))\n",
        "  X_test = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], X_test))\n",
        "\n",
        "  # remove numbers\n",
        "  import re\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # remove user, url and other commond words\n",
        "  forbidden_words = [\"url\", \"user\"]\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # # remove duplicates from the training data\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # join back the tweets into a phrase\n",
        "  X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "  tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n"
      ],
      "metadata": {
        "id": "NMpcCsG3TarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(tweets[i])\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "for i in range(20):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMoLGO9XT17_",
        "outputId": "225781d7-39aa-4662-da5f-37db462a44aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "wish i could be out all night tonight ! <user>\n",
            "<user> i got kicked out the wgm\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "why is she so perfect <url>\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "<user> i'm white . #aw\n",
            "<user> dan i love and miss you ! don't be sad #wheresthegeneral\n",
            "so many wonderful building in dc but still miss you <user>\n",
            "<user> it's annoying because i secretly find it so good ...\n",
            "the post-boom in spanish american fiction ( suny series in latin american and iberian thought and culture what ... <url>\n",
            "layers of the heart ( paperback this journey was inspired by a recent robbery that took place in the united sta ... <url>\n",
            "layers of the heart ( paperback this journey was inspired by a recent robbery that took place in the united sta ... <url>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n",
            "watch some of y'all dumb asses get lock up today #happy420\n",
            "obsessed with #phasell <user> you killed it ! ! ! best album ever love yew roycee ! ! : * rt me\n",
            "<user> robert de niro is not gay .. but with a name like lewy\n",
            "<user> canada have to do it in grade 12 . but since we don't have grade 12 here\n",
            "<user> please say hi to denmark ! that would be amazing ! ( <user> live on <url>\n",
            "finally am home now\n",
            "3x3 custom picture frame / poster frame 1.2 \" wide complete gold frame ( 2380763 9gd this frame is manufactured i ... <url>\n",
            "s / o to my new followers . mention me for a followback boo\n",
            "<user> yep\n",
            "nhl's bettman : suspension criticism ' gamesmanship ' ( the associated press new york ( ap ) nhl ... <url> #predators #nhl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the training data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(tweets, labels, random_state=84)"
      ],
      "metadata": {
        "id": "4YqG3R8WT4bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save X data to a file\n",
        "def save_X_data(file_name, X):\n",
        "  f = open(DATA_PATH.format(file_name), \"w\")\n",
        "  for element in X:\n",
        "      # f.write(\" \".join(element) + \"\\n\")\n",
        "      f.write(element + \"\\n\")\n",
        "  f.close()\n",
        "\n",
        "save_X_data(\"X_train_processed_bert.txt\" if is_bert_preprocessing_enabled else \"X_train_processed.txt\", X_train)\n",
        "save_X_data(\"X_test_processed_bert.txt\" if is_bert_preprocessing_enabled else \"X_test_processed.txt\", X_test)"
      ],
      "metadata": {
        "id": "CKAb04McUEh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save labels to file\n",
        "f = open(DATA_PATH.format(\"y_train.txt\"), \"w\")\n",
        "for label in y_train:\n",
        "  f.write(str(label) + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1v4OYiq0Wvda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGfqcvEHhRj"
      },
      "source": [
        "# INITIAL EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YGXIVtwF1C7",
        "outputId": "42363252-cdb0-4d7c-da85-fe9420b7d3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our dataset there are 1250000 negative tweets\n",
            "In our dataset there are 1250000 positive tweets\n"
          ]
        }
      ],
      "source": [
        "# get number of positive and negative tweets\n",
        "print(f\"In our dataset there are {(labels == 0).sum()} negative tweets\")\n",
        "print(f\"In our dataset there are {(labels == 1).sum()} positive tweets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivQ_UyYXD1zi",
        "outputId": "fc6ca0f0-2554-4a31-9e66-3a745f17dd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average length of tweets with a negative sentiment is: 84.0923984\n",
            "The average length of tweets with a positive sentiment is: 67.835752\n"
          ]
        }
      ],
      "source": [
        "# get the average length of positive and negative examples\n",
        "def get_average_length(target_label):\n",
        "  len_tweets = []\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      len_tweets.append(len(tweet))\n",
        "  \n",
        "  return np.array(len_tweets).mean()\n",
        "\n",
        "# NOTE: postive tweets seem to be longer on average than negative tweets.\n",
        "# TODO: check if this thing can be used at our advantange in training\n",
        "print(f\"The average length of tweets with a negative sentiment is: {get_average_length(0)}\")\n",
        "print(f\"The average length of tweets with a positive sentiment is: {get_average_length(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get most frequent words in positive and negative examples\n",
        "def count_words(target_label):\n",
        "  words_occurrences = {}\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      for word in tweet:\n",
        "        words_occurrences[word] = words_occurrences.get(word, 0) + 1\n",
        "\n",
        "  return words_occurrences\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tweets = list(map(lambda x : x.split, tweets))\n",
        "# negative tweets\n",
        "print(\"NEGATIVE TWEETS:\")\n",
        "negative = count_words(0)\n",
        "print(dict(Counter(negative).most_common(100)))\n",
        "# print uniques words in the negative tweets\n",
        "print(len(negative))\n",
        "\n",
        "# positive tweets\n",
        "print(\"\\n\\n\\n POSITIVE TWEETS:\")\n",
        "positive = count_words(1)\n",
        "print(dict(Counter(positive).most_common(100)))\n",
        "# print uniques words in the positive tweets\n",
        "print(len(positive))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTDyENY06On1",
        "outputId": "870cadee-f54b-4b2f-88f6-074c27d53c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE TWEETS:\n",
            "{' ': 20178690, 'e': 8558475, 't': 6041262, 'a': 5942525, 'o': 5902744, 'i': 5340092, 'r': 5141301, 's': 5120700, 'n': 4565219, 'l': 3757830, 'h': 3265654, 'u': 3196129, 'd': 2633209, 'm': 2422797, 'c': 2265983, 'y': 1825793, '.': 1786193, 'g': 1716708, 'w': 1666053, 'p': 1635392, 'f': 1424695, 'b': 1369047, 'k': 1041608, '<': 1041060, '>': 1022710, 'v': 766399, '(': 552651, \"'\": 468998, ',': 406069, '!': 368087, '1': 321188, '-': 316038, '0': 314103, 'x': 294188, '2': 280944, 'j': 196039, '3': 176580, ':': 157767, '5': 153606, '\"': 150624, '#': 147696, '?': 140983, '4': 128185, '6': 119481, 'z': 115971, '/': 109841, '8': 96389, '7': 85965, ')': 84336, '9': 81105, 'q': 57020, '&': 44897, '*': 25457, '_': 14591, '|': 13706, ';': 9660, '+': 8010, '[': 6797, '$': 6661, ']': 6312, '=': 6065, '%': 5482, '~': 5124, '^': 3690, '@': 2238, '\\\\': 2049, '`': 1211, '{': 698, '}': 633, '“': 28, '®': 23, '’': 13, '×': 5, '°': 4, '♥': 4, '\\x13': 2, '‘': 2, '”': 2, '\\t': 2, '►': 2, '\\x0e': 1, 'ñ': 1, 'ü': 1, '√': 1, '☼': 1, '\\x18': 1, '•': 1, '\\x05': 1}\n",
            "88\n",
            "\n",
            "\n",
            "\n",
            " POSITIVE TWEETS:\n",
            "{' ': 16676948, 'e': 7163884, 'o': 5337069, 't': 5010834, 'a': 4643161, 's': 4302909, 'i': 3986587, 'r': 3863362, 'n': 3540423, 'u': 3156147, 'h': 3042906, 'l': 2939723, 'y': 2044151, 'd': 1858044, 'm': 1800045, 'w': 1573769, 'g': 1475596, 'c': 1204836, '<': 1178963, '>': 1154753, 'f': 1129711, 'b': 1008703, 'p': 961922, 'k': 910357, '.': 832046, '!': 633601, 'v': 590552, \"'\": 468303, ',': 345951, 'x': 256059, '#': 217073, '?': 193768, 'j': 190432, '\"': 115324, ')': 108237, ':': 83747, 'z': 78784, '3': 76741, '-': 75838, '1': 70125, '0': 68065, '2': 61065, '&': 53821, '(': 36480, '/': 34088, '4': 30451, '*': 28171, 'q': 25767, '5': 24907, '9': 15324, '_': 13907, '8': 13590, '7': 13583, '6': 13479, ';': 11428, '^': 9808, '=': 6109, '~': 5523, '+': 4722, '$': 4160, '@': 3663, ']': 3113, '[': 2976, '|': 2443, '%': 1983, '\\\\': 1955, '`': 1214, '}': 790, '{': 709, '\\x13': 3, '\\x05': 2, '\\x17': 2, '\\x10': 1, 'î': 1, '\\x08': 1, '\\x0e': 1, '\\x11': 1}\n",
            "77\n"
          ]
        }
      ]
    }
  ]
}