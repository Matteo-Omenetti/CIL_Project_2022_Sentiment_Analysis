{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_Matteo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1jVrOExYS96j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s03gfomSz1n",
        "outputId": "439a5b20-317f-45e8-d2fa-77e819dc96d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298Gg8IFS9gV",
        "outputId": "9771185a-fcb5-40e6-9499-5b60c850e02c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLIkk5YGTSzE",
        "outputId": "82c9980b-c76d-4010-bee5-7c8a8b00d829"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and global variables\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'\n",
        "\n",
        "# if set to True the preprocessing for the bert model will be done, otherswise \n",
        "# the preprocessig for w2v and Tf-idf will be performed\n",
        "is_bert_preprocessing_enabled = False"
      ],
      "metadata": {
        "id": "OrOVITskTH7V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "0ldj0wg4TUAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "\n",
        "def load_tweets(filename, label):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweets.append(line.rstrip())\n",
        "            labels.append(label)\n",
        "\n",
        "# load training tweets   \n",
        "load_tweets(DATA_PATH.format('train_neg_full.txt'), 0)\n",
        "load_tweets(DATA_PATH.format('train_pos_full.txt'), 1)\n",
        "# Convert to NumPy array to facilitate indexing\n",
        "print(f'{len(tweets)} training/dev tweets loaded')\n",
        "tweets = np.array(tweets)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# load the test file\n",
        "f = open(DATA_PATH.format('test_data.txt'), 'r', encoding='utf-8')\n",
        "X_test = []\n",
        "for line in f:\n",
        "  X_test.append(\",\".join(line.split(',')[1:]).strip())\n",
        "X_test = np.array(X_test)\n",
        "print(f'{len(X_test)} test tweets loaded')\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(tweets[i])\n",
        "    print(labels[i])\n",
        "\n",
        "for i in range(10):\n",
        "  print(X_test[i])\n",
        "\n",
        "print(len(tweets))"
      ],
      "metadata": {
        "id": "Pg1VSXtvTQo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19229e18-f7a2-47af-8a21-de284482465f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500000 training/dev tweets loaded\n",
            "10000 test tweets loaded\n",
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "0\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "0\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "0\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "0\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "0\n",
            "wish i could be out all night tonight ! <user>\n",
            "0\n",
            "<user> i got kicked out the wgm\n",
            "0\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "0\n",
            "why is she so perfect <url>\n",
            "0\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "0\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv , i always wake up with a headache\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n",
            "2500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "0-b6wdk3TbVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for bert\n",
        "if is_bert_preprocessing_enabled:\n",
        "\n",
        "  # Lowercase sentence\n",
        "  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  # # remove hashtags\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), X_test))\n",
        "\n",
        "  # remove duplicates from the training data (decreases accuracy)\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # split each tweet into separate words\n",
        "  # tweets = list(map(lambda tweet : tweet.split(), tweets))\n",
        "  # X_test = list(map(lambda tweet : tweet.split(), X_test))\n",
        "  \n",
        "  # remove user, url and other commond words (decreases accuracy)\n",
        "  # forbidden_words = [\"<url>\", \"<user>\"]\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # from nltk.tokenize import RegexpTokenizer\n",
        "  # # Tokenize sentence\n",
        "  # tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # Remove stopwords (decrease accuracy)\n",
        "  # from nltk.corpus import stopwords\n",
        "  # stopwords_set = stopwords.words('english')\n",
        "  # tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  # X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  # remove numbers (decreases accuracy)\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # # join to back the tweets into a phrase\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "CfBSuzHlLR9r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for non bert\n",
        "if not is_bert_preprocessing_enabled:\n",
        "  # Lowercase sentence\n",
        "  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  from nltk.tokenize import RegexpTokenizer\n",
        "  # Tokenize sentence\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # remove hashtags\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], X_test))\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "  # Remove stopwords\n",
        "  stopwords_set = stopwords.words('english')\n",
        "  tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  # Lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweets = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], tweets))\n",
        "  X_test = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], X_test))\n",
        "\n",
        "  # remove numbers\n",
        "  import re\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # remove user, url and other commond words\n",
        "  forbidden_words = [\"url\", \"user\"]\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # # remove duplicates from the training data\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # join back the tweets into a phrase\n",
        "  X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "  tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n"
      ],
      "metadata": {
        "id": "NMpcCsG3TarA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(tweets[i])\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "for i in range(20):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMoLGO9XT17_",
        "outputId": "5c09ee9b-a61d-41ed-a364-30f49827ffbe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vinco tresorpack difficulty object disassemble reassemble wooden piece beautiful wo\n",
            "glad dot taks tomorrow thankful startho\n",
            "v celtic regular season fucked play playoff\n",
            "could actually kill girl sorry\n",
            "find hard believe im afraid\n",
            "wish could night tonight\n",
            "got kicked wgm\n",
            "rt yes u tell lip closed okay\n",
            "perfect\n",
            "hi harry u havea good time au didnt get see u maybe next year follow back u would bea dreamcome truex\n",
            "introduction programming c edition solid foundation basic c programming\n",
            "introduction programming c edition solid foundation basic c programming\n",
            "introduction programming c edition solid foundation basic c programming\n",
            "white aw\n",
            "dan love miss sad wheresthegeneral\n",
            "many wonderful building dc still miss\n",
            "annoying secretly find good\n",
            "post boom spanish american fiction suny series latin american iberian thought culture\n",
            "layer heart paperback journey inspired recent robbery took place united sta\n",
            "layer heart paperback journey inspired recent robbery took place united sta\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sea doo pro sea scooter sport portable sea doo seascootersave air stay longer water\n",
            "shuck well work week come cheer oh put battery calculator\n",
            "cant stay away bug thats baby\n",
            "lol im perfectly fine contagious anymore lmao\n",
            "whenever fall asleep watching tv always wake headache\n",
            "need get rid thing scare lol need car either need driver ed\n",
            "whatever terrible mood\n",
            "yes rt thanks jordan love gonna call later\n",
            "friend text check last night\n",
            "followback please ur unitytour come europe sweden\n",
            "watch dumb ass get lock today happy420\n",
            "obsessed phasell killed best album ever love yew roycee rt\n",
            "robert de niro gay name like lewy understand hahahahha sherlock\n",
            "canada grade since grade suck\n",
            "please say hi denmark would amazing live\n",
            "finally home\n",
            "custom picture frame poster frame wide complete gold frame frame manufactured\n",
            "new follower mention followback boo\n",
            "yep look like best team stay proper form see bit\n",
            "nhl bettman suspension criticism gamesmanship associated press new york ap nhl predator nhl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the training data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(tweets, labels, random_state=84)"
      ],
      "metadata": {
        "id": "4YqG3R8WT4bM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save X data to a file\n",
        "def save_X_data(file_name, X):\n",
        "  f = open(DATA_PATH.format(file_name), \"w\")\n",
        "  for element in X:\n",
        "      # f.write(\" \".join(element) + \"\\n\")\n",
        "      f.write(element + \"\\n\")\n",
        "  f.close()\n",
        "\n",
        "save_X_data(\"X_train_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_train_processed.txt\", X_train)\n",
        "save_X_data(\"X_test_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_test_processed.txt\", X_test)"
      ],
      "metadata": {
        "id": "CKAb04McUEh5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save labels to file\n",
        "f = open(DATA_PATH.format(\"y_train.txt\"), \"w\")\n",
        "for label in y_train:\n",
        "  f.write(str(label) + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1v4OYiq0Wvda"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGfqcvEHhRj"
      },
      "source": [
        "# INITIAL EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YGXIVtwF1C7",
        "outputId": "0aa3f2fc-a9ac-42b2-8687-1fc4b848857b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our dataset there are 1250000 negative tweets\n",
            "In our dataset there are 1250000 positive tweets\n"
          ]
        }
      ],
      "source": [
        "# get number of positive and negative tweets\n",
        "print(f\"In our dataset there are {(labels == 0).sum()} negative tweets\")\n",
        "print(f\"In our dataset there are {(labels == 1).sum()} positive tweets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivQ_UyYXD1zi",
        "outputId": "0ac5b375-2101-4dcd-8aef-4d18db91d745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average length of tweets with a negative sentiment is: 51.1157288\n",
            "The average length of tweets with a positive sentiment is: 37.0986992\n"
          ]
        }
      ],
      "source": [
        "# get the average length of positive and negative examples\n",
        "def get_average_length(target_label):\n",
        "  len_tweets = []\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      len_tweets.append(len(tweet))\n",
        "  \n",
        "  return np.array(len_tweets).mean()\n",
        "\n",
        "# NOTE: postive tweets seem to be longer on average than negative tweets.\n",
        "print(f\"The average length of tweets with a negative sentiment is: {get_average_length(0)}\")\n",
        "print(f\"The average length of tweets with a positive sentiment is: {get_average_length(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get most frequent words in positive and negative examples\n",
        "def count_words(target_label):\n",
        "  words_occurrences = {}\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      for word in tweet:\n",
        "        words_occurrences[word] = words_occurrences.get(word, 0) + 1\n",
        "\n",
        "  return words_occurrences\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tweets = list(map(lambda x : x.split(), tweets))\n",
        "# negative tweets\n",
        "print(\"NEGATIVE TWEETS:\")\n",
        "negative = count_words(0)\n",
        "print(dict(Counter(negative).most_common(100)))\n",
        "# print uniques words in the negative tweets\n",
        "print(len(negative))\n",
        "\n",
        "# positive tweets\n",
        "print(\"\\n\\n\\n POSITIVE TWEETS:\")\n",
        "positive = count_words(1)\n",
        "print(dict(Counter(positive).most_common(100)))\n",
        "# print uniques words in the positive tweets\n",
        "print(len(positive))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTDyENY06On1",
        "outputId": "21788cd4-d499-4894-e3a0-790c802bbde4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE TWEETS:\n",
            "{'frame': 102464, 'rt': 56658, 'like': 55673, 'u': 52488, 'get': 52067, 'go': 50980, 'one': 48103, 'want': 45193, 'paperback': 43726, 'miss': 42609, 'know': 42561, 'day': 38829, 'really': 36500, 'time': 35358, 'love': 34868, 'pack': 32850, 'black': 32752, 'see': 32029, 'x': 31429, 'please': 31428, 'complete': 31283, 'back': 31109, 'picture': 30957, 'need': 30432, 'lol': 30146, 'wish': 29888, 'got': 28315, 'im': 28047, 'going': 27808, 'today': 27585, 'wide': 27324, 'feel': 26855, 'poster': 26620, 'new': 26317, 'custom': 26146, 'make': 24955, 'sad': 24903, 'think': 24530, 'work': 24335, 'good': 24202, 'come': 23695, 'never': 23377, 'much': 23338, 'still': 23055, 'edition': 22846, 'hate': 22774, 'follow': 21341, 'home': 20523, 'wanna': 20324, 'would': 20053, 'sorry': 19781, 'friend': 19595, 'right': 19427, 'life': 19338, 'year': 19174, 'book': 18919, 'school': 18628, 'bad': 18589, 'last': 18375, 'oh': 18193, 'even': 17355, 'night': 17016, 'could': 16928, 'hardcover': 16884, 'cry': 16873, 'well': 16763, 'set': 16661, 'gonna': 16432, 'people': 16321, 'tomorrow': 15879, 'cd': 15771, 'thing': 15577, 'dvd': 15502, 'way': 15439, 'sleep': 15390, 'say': 15374, 'phone': 15225, 'world': 15177, 'haha': 14756, 'girl': 14704, 'omg': 14682, 'series': 14586, 'white': 14545, 'take': 14471, 'man': 14235, 'dont': 14144, 'video': 13960, 'wood': 13953, 'week': 13901, 'case': 13810, 'look': 13668, 'someone': 13615, 'though': 13576, 'baby': 13568, 'tweet': 13400, 'hope': 13259, 'best': 12972, 'game': 12819, 'always': 12667, 'audio': 12427}\n",
            "290669\n",
            "\n",
            "\n",
            "\n",
            " POSITIVE TWEETS:\n",
            "{'love': 95837, 'rt': 87663, 'u': 76740, 'good': 72723, 'follow': 72095, 'like': 67511, 'lol': 67275, 'thanks': 63631, 'get': 62675, 'day': 59774, 'know': 58092, 'one': 53893, 'please': 50787, 'go': 44505, 'haha': 43321, 'see': 42012, 'back': 40978, 'time': 39416, 'thank': 37168, 'x': 36757, 'got': 35625, 'make': 35242, 'today': 35080, 'want': 34879, 'think': 32595, 'happy': 32063, 'well': 30897, 'im': 30095, 'going': 28933, 'girl': 28651, 'best': 27018, 'really': 27001, 'would': 26842, 'much': 25912, 'night': 25268, 'come': 25131, 'need': 25041, 'hope': 24448, 'say': 24133, 'new': 24043, 'xx': 24033, 'tweet': 23744, 'tomorrow': 22950, 'great': 22660, 'yeah': 22594, 'oh': 22517, 'hey': 22394, 'friend': 22282, 'guy': 22175, 'let': 21779, 'yes': 21569, 'look': 21340, 'people': 20541, 'wait': 20379, 'right': 20119, 'thing': 19839, 'birthday': 19749, 'better': 18978, 'nice': 18894, 'always': 18421, 'still': 18029, 'way': 17690, 'gonna': 17623, 'work': 17260, 'morning': 17006, 'tonight': 16878, 'xxx': 16190, 'life': 16186, 'never': 16127, 'twitter': 16022, 'take': 15935, 'amazing': 15652, 'next': 14999, 'school': 14487, 'feel': 14334, 'boy': 14317, 'beautiful': 14303, 'baby': 14167, 'could': 14165, 'following': 13972, 'everyone': 13770, 'cute': 13745, 'okay': 13717, 'fun': 13605, 'give': 13514, 'wanna': 13418, 'sure': 13301, 'first': 13279, 'though': 13244, 'live': 13202, 'tell': 13034, 'home': 12918, 'ur': 12836, 'last': 12797, 'pretty': 12644, 'p': 12536, 'hahaha': 12436, 'song': 12310, 'fan': 12179, 'even': 12149}\n",
            "238706\n"
          ]
        }
      ]
    }
  ]
}