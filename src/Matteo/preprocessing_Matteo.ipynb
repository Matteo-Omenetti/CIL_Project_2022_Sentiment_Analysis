{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_Matteo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1jVrOExYS96j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s03gfomSz1n",
        "outputId": "a4d9dc6e-f091-49d3-9e98-4169507f1db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298Gg8IFS9gV",
        "outputId": "3e64dea1-efda-46ad-a511-7dd2a2a8539d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLIkk5YGTSzE",
        "outputId": "813805f2-e04d-4e2b-94af-0236617d904c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and global variables\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'"
      ],
      "metadata": {
        "id": "OrOVITskTH7V"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "0ldj0wg4TUAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "\n",
        "def load_tweets(filename, label):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweets.append(line.rstrip())\n",
        "            labels.append(label)\n",
        "\n",
        "# load training tweets   \n",
        "load_tweets(DATA_PATH.format('train_neg_full.txt'), 0)\n",
        "load_tweets(DATA_PATH.format('train_pos_full.txt'), 1)\n",
        "# Convert to NumPy array to facilitate indexing\n",
        "print(f'{len(tweets)} training/dev tweets loaded')\n",
        "tweets = np.array(tweets)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# load the test file\n",
        "f = open(DATA_PATH.format('test_data.txt'), 'r', encoding='utf-8')\n",
        "X_test = []\n",
        "for line in f:\n",
        "  X_test.append(line.split(',')[1].rstrip())\n",
        "X_test = np.array(X_test)\n",
        "print(f'{len(X_test)} test tweets loaded')\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(tweets[i])\n",
        "    print(labels[i])\n",
        "\n",
        "for i in range(10):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg1VSXtvTQo1",
        "outputId": "676c4f16-b97e-4f27-a5cc-0f9bb244e5e0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500000 training/dev tweets loaded\n",
            "10000 test tweets loaded\n",
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "0\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "0\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "0\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "0\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "0\n",
            "wish i could be out all night tonight ! <user>\n",
            "0\n",
            "<user> i got kicked out the wgm\n",
            "0\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "0\n",
            "why is she so perfect <url>\n",
            "0\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "0\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "0-b6wdk3TbVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing sentence\n",
        "tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "# Tokenizing sentence\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "# Removing stopwords\n",
        "stopwords_set = stopwords.words('english')\n",
        "tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tweets = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], tweets))\n",
        "X_test = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], X_test))\n",
        "\n",
        "# remove numbers\n",
        "import re\n",
        "tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "# remove user, url and other commond words\n",
        "forbidden_words = [\"url\", \"user\"]\n",
        "tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n"
      ],
      "metadata": {
        "id": "NMpcCsG3TarA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(tweets[i])\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "for i in range(20):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMoLGO9XT17_",
        "outputId": "0640d29d-4309-4baf-af12-b8dfc478e774"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vinco', 'tresorpack', 'difficulty', 'object', 'disassemble', 'reassemble', 'wooden', 'piece', 'beautiful', 'wo']\n",
            "['glad', 'dot', 'taks', 'tomorrow', 'thankful', 'startho']\n",
            "['v', 'celtic', 'regular', 'season', 'fucked', 'play', 'playoff']\n",
            "['could', 'actually', 'kill', 'girl', 'sorry']\n",
            "['find', 'hard', 'believe', 'im', 'afraid']\n",
            "['wish', 'could', 'night', 'tonight']\n",
            "['got', 'kicked', 'wgm']\n",
            "['rt', 'yes', 'u', 'tell', 'lip', 'closed', 'okay']\n",
            "['perfect']\n",
            "['hi', 'harry', 'u', 'havea', 'good', 'time', 'au', 'didnt', 'get', 'see', 'u', 'maybe', 'next', 'year', 'follow', 'back', 'u', 'would', 'bea', 'dreamcome', 'truex']\n",
            "['introduction', 'programming', 'c', 'edition', 'solid', 'foundation', 'basic', 'c', 'programming']\n",
            "['introduction', 'programming', 'c', 'edition', 'solid', 'foundation', 'basic', 'c', 'programming']\n",
            "['introduction', 'programming', 'c', 'edition', 'solid', 'foundation', 'basic', 'c', 'programming']\n",
            "['white', 'aw']\n",
            "['dan', 'love', 'miss', 'sad', 'wheresthegeneral']\n",
            "['many', 'wonderful', 'building', 'dc', 'still', 'miss']\n",
            "['annoying', 'secretly', 'find', 'good']\n",
            "['post', 'boom', 'spanish', 'american', 'fiction', 'suny', 'series', 'latin', 'american', 'iberian', 'thought', 'culture']\n",
            "['layer', 'heart', 'paperback', 'journey', 'inspired', 'recent', 'robbery', 'took', 'place', 'united', 'sta']\n",
            "['layer', 'heart', 'paperback', 'journey', 'inspired', 'recent', 'robbery', 'took', 'place', 'united', 'sta']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['sea', 'doo', 'pro', 'sea', 'scooter', 'sport', 'portable', 'sea', 'doo', 'seascootersave', 'air']\n",
            "['shuck', 'well', 'work', 'week', 'come', 'cheer', 'oh', 'put', 'battery', 'calculator']\n",
            "['cant', 'stay', 'away', 'bug', 'thats', 'baby']\n",
            "['lol', 'im', 'perfectly', 'fine', 'contagious', 'anymore', 'lmao']\n",
            "['whenever', 'fall', 'asleep', 'watching', 'tv']\n",
            "['need', 'get', 'rid', 'thing', 'scare', 'lol', 'need', 'car', 'either', 'need', 'driver', 'ed']\n",
            "['whatever', 'terrible', 'mood']\n",
            "['yes', 'rt', 'thanks', 'jordan']\n",
            "['friend', 'text', 'check', 'last', 'night']\n",
            "['followback', 'please', 'ur', 'unitytour', 'come', 'europe', 'sweden']\n",
            "['watch', 'dumb', 'ass', 'get', 'lock', 'today', 'happy420']\n",
            "['obsessed', 'phasell', 'killed', 'best', 'album', 'ever', 'love', 'yew', 'roycee', 'rt']\n",
            "['robert', 'de', 'niro', 'gay', 'name', 'like', 'lewy']\n",
            "['canada', 'grade', 'since', 'grade']\n",
            "['please', 'say', 'hi', 'denmark', 'would', 'amazing', 'live']\n",
            "['finally', 'home']\n",
            "['custom', 'picture', 'frame', 'poster', 'frame', 'wide', 'complete', 'gold', 'frame', 'frame', 'manufactured']\n",
            "['new', 'follower', 'mention', 'followback', 'boo']\n",
            "['yep']\n",
            "['nhl', 'bettman', 'suspension', 'criticism', 'gamesmanship', 'associated', 'press', 'new', 'york', 'ap', 'nhl', 'predator', 'nhl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the training data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(tweets, labels, random_state=84)"
      ],
      "metadata": {
        "id": "4YqG3R8WT4bM"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save X data to a file\n",
        "def save_X_data(file_name, X):\n",
        "  f = open(DATA_PATH.format(file_name), \"w\")\n",
        "  for element in X:\n",
        "      f.write(\" \".join(element) + \"\\n\")\n",
        "  f.close()\n",
        "\n",
        "save_X_data(\"X_train_processed.txt\", X_train)\n",
        "save_X_data(\"X_test_processed.txt\", X_test)"
      ],
      "metadata": {
        "id": "CKAb04McUEh5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save labels to file\n",
        "f = open(DATA_PATH.format(\"y_train.txt\"), \"w\")\n",
        "for label in y_train:\n",
        "  f.write(str(label) + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1v4OYiq0Wvda"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGfqcvEHhRj"
      },
      "source": [
        "# INITIAL EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YGXIVtwF1C7",
        "outputId": "3d25b308-5a1b-4d16-e959-76c7ac775994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our dataset there are 1250000 negative tweets\n",
            "In our dataset there are 1250000 positive tweets\n"
          ]
        }
      ],
      "source": [
        "# get number of positive and negative tweets\n",
        "print(f\"In our dataset there are {(labels == 0).sum()} negative tweets\")\n",
        "print(f\"In our dataset there are {(labels == 1).sum()} positive tweets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivQ_UyYXD1zi",
        "outputId": "f5fb8b93-86dc-4ec2-e88b-c00bcd5b4e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average length of tweets with a negative sentiment is: 8.3099912\n",
            "The average length of tweets with a positive sentiment is: 6.3471352\n"
          ]
        }
      ],
      "source": [
        "# get the average length of positive and negative examples\n",
        "def get_average_length(target_label):\n",
        "  len_tweets = []\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      len_tweets.append(len(tweet))\n",
        "  \n",
        "  return np.array(len_tweets).mean()\n",
        "\n",
        "# NOTE: postive tweets seem to be longer on average than negative tweets.\n",
        "# TODO: check if this thing can be used at our advantange in training\n",
        "print(f\"The average length of tweets with a negative sentiment is: {get_average_length(0)}\")\n",
        "print(f\"The average length of tweets with a positive sentiment is: {get_average_length(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get most frequent words in positive and negative examples\n",
        "def count_words(target_label):\n",
        "  words_occurrences = {}\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      for word in tweet:\n",
        "        words_occurrences[word] = words_occurrences.get(word, 0) + 1\n",
        "\n",
        "  return words_occurrences\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# negative tweets\n",
        "print(\"NEGATIVE TWEETS:\")\n",
        "negative = count_words(0)\n",
        "print(dict(Counter(negative).most_common(100)))\n",
        "# print uniques words in the negative tweets\n",
        "print(len(negative))\n",
        "\n",
        "# positive tweets\n",
        "print(\"\\n\\n\\n POSITIVE TWEETS:\")\n",
        "positive = count_words(1)\n",
        "print(dict(Counter(positive).most_common(100)))\n",
        "# print uniques words in the positive tweets\n",
        "print(len(positive))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTDyENY06On1",
        "outputId": "f5ecca32-69ae-42bb-a7fd-80edc44451c7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE TWEETS:\n",
            "{'frame': 102464, 'rt': 56658, 'like': 55673, 'u': 52488, 'get': 52067, 'go': 50980, 'one': 48103, 'want': 45193, 'paperback': 43726, 'miss': 42609, 'know': 42561, 'day': 38829, 'really': 36500, 'time': 35358, 'love': 34868, 'pack': 32850, 'black': 32752, 'see': 32029, 'x': 31429, 'please': 31428, 'complete': 31283, 'back': 31109, 'picture': 30957, 'need': 30432, 'lol': 30146, 'wish': 29888, 'got': 28315, 'im': 28047, 'going': 27808, 'today': 27585, 'wide': 27324, 'feel': 26855, 'poster': 26620, 'new': 26317, 'custom': 26146, 'make': 24955, 'sad': 24903, 'think': 24530, 'work': 24335, 'good': 24202, 'come': 23695, 'never': 23377, 'much': 23338, 'still': 23055, 'edition': 22846, 'hate': 22774, 'follow': 21341, 'home': 20523, 'wanna': 20324, 'would': 20053, 'sorry': 19781, 'friend': 19595, 'right': 19427, 'life': 19338, 'year': 19174, 'book': 18919, 'school': 18628, 'bad': 18589, 'last': 18375, 'oh': 18193, 'even': 17355, 'night': 17016, 'could': 16928, 'hardcover': 16884, 'cry': 16873, 'well': 16763, 'set': 16661, 'gonna': 16432, 'people': 16321, 'tomorrow': 15879, 'cd': 15771, 'thing': 15577, 'dvd': 15502, 'way': 15439, 'sleep': 15390, 'say': 15374, 'phone': 15225, 'world': 15177, 'haha': 14756, 'girl': 14704, 'omg': 14682, 'series': 14586, 'white': 14545, 'take': 14471, 'man': 14235, 'dont': 14144, 'video': 13960, 'wood': 13953, 'week': 13901, 'case': 13810, 'look': 13668, 'someone': 13615, 'though': 13576, 'baby': 13568, 'tweet': 13400, 'hope': 13259, 'best': 12972, 'game': 12819, 'always': 12667, 'audio': 12427}\n",
            "290669\n",
            "\n",
            "\n",
            "\n",
            " POSITIVE TWEETS:\n",
            "{'love': 95837, 'rt': 87663, 'u': 76740, 'good': 72723, 'follow': 72095, 'like': 67511, 'lol': 67275, 'thanks': 63631, 'get': 62675, 'day': 59774, 'know': 58092, 'one': 53893, 'please': 50787, 'go': 44505, 'haha': 43321, 'see': 42012, 'back': 40978, 'time': 39416, 'thank': 37168, 'x': 36757, 'got': 35625, 'make': 35242, 'today': 35080, 'want': 34879, 'think': 32595, 'happy': 32063, 'well': 30897, 'im': 30095, 'going': 28933, 'girl': 28651, 'best': 27018, 'really': 27001, 'would': 26842, 'much': 25912, 'night': 25268, 'come': 25131, 'need': 25041, 'hope': 24448, 'say': 24133, 'new': 24043, 'xx': 24033, 'tweet': 23744, 'tomorrow': 22950, 'great': 22660, 'yeah': 22594, 'oh': 22517, 'hey': 22394, 'friend': 22282, 'guy': 22175, 'let': 21779, 'yes': 21569, 'look': 21340, 'people': 20541, 'wait': 20379, 'right': 20119, 'thing': 19839, 'birthday': 19749, 'better': 18978, 'nice': 18894, 'always': 18421, 'still': 18029, 'way': 17690, 'gonna': 17623, 'work': 17260, 'morning': 17006, 'tonight': 16878, 'xxx': 16190, 'life': 16186, 'never': 16127, 'twitter': 16022, 'take': 15935, 'amazing': 15652, 'next': 14999, 'school': 14487, 'feel': 14334, 'boy': 14317, 'beautiful': 14303, 'baby': 14167, 'could': 14165, 'following': 13972, 'everyone': 13770, 'cute': 13745, 'okay': 13717, 'fun': 13605, 'give': 13514, 'wanna': 13418, 'sure': 13301, 'first': 13279, 'though': 13244, 'live': 13202, 'tell': 13034, 'home': 12918, 'ur': 12836, 'last': 12797, 'pretty': 12644, 'p': 12536, 'hahaha': 12436, 'song': 12310, 'fan': 12179, 'even': 12149}\n",
            "238706\n"
          ]
        }
      ]
    }
  ]
}