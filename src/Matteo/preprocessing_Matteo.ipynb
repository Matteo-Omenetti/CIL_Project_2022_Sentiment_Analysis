{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_Matteo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wLGfqcvEHhRj"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1jVrOExYS96j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s03gfomSz1n",
        "outputId": "d66844e7-385e-43c2-cd2a-461543d55b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298Gg8IFS9gV",
        "outputId": "47f2d30e-ab22-4310-eced-41c950592441"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLIkk5YGTSzE",
        "outputId": "0d3db2af-b239-4e95-9753-511da21a2fbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and global variables\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'\n",
        "\n",
        "# if set to True the preprocessing for the bert model will be done, otherswise \n",
        "# the preprocessig for w2v and Tf-idf will be performed\n",
        "is_bert_preprocessing_enabled = True"
      ],
      "metadata": {
        "id": "OrOVITskTH7V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "0ldj0wg4TUAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "labels = []\n",
        "\n",
        "def load_tweets(filename, label):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tweets.append(line.rstrip())\n",
        "            labels.append(label)\n",
        "\n",
        "# load training tweets   \n",
        "load_tweets(DATA_PATH.format('train_neg.txt'), 0)\n",
        "load_tweets(DATA_PATH.format('train_pos.txt'), 1)\n",
        "# Convert to NumPy array to facilitate indexing\n",
        "print(f'{len(tweets)} training/dev tweets loaded')\n",
        "tweets = np.array(tweets)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# load the test file\n",
        "f = open(DATA_PATH.format('test_data.txt'), 'r', encoding='utf-8')\n",
        "X_test = []\n",
        "for line in f:\n",
        "  X_test.append(\",\".join(line.split(',')[1:]).strip())\n",
        "X_test = np.array(X_test)\n",
        "print(f'{len(X_test)} test tweets loaded')\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(tweets[i])\n",
        "    print(labels[i])\n",
        "\n",
        "for i in range(10):\n",
        "  print(X_test[i])\n",
        "\n",
        "print(len(tweets))"
      ],
      "metadata": {
        "id": "Pg1VSXtvTQo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9eaa318-9584-4421-8a9d-fb6d16ff3bab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000 training/dev tweets loaded\n",
            "10000 test tweets loaded\n",
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "0\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "0\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "0\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "0\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "0\n",
            "wish i could be out all night tonight ! <user>\n",
            "0\n",
            "<user> i got kicked out the wgm\n",
            "0\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "0\n",
            "why is she so perfect <url>\n",
            "0\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "0\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv , i always wake up with a headache\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n",
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "0-b6wdk3TbVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for bert\n",
        "if is_bert_preprocessing_enabled:\n",
        "\n",
        "  # Lowercase sentence\n",
        "  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  # # remove hashtags\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : re.sub(\"#\\w+\", \"\", tweet), X_test))\n",
        "\n",
        "  # remove duplicates from the training data (decreases accuracy)\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # split each tweet into separate words\n",
        "  # tweets = list(map(lambda tweet : tweet.split(), tweets))\n",
        "  # X_test = list(map(lambda tweet : tweet.split(), X_test))\n",
        "  \n",
        "  # remove user, url and other commond words (decreases accuracy)\n",
        "  # forbidden_words = [\"<url>\", \"<user>\"]\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # from nltk.tokenize import RegexpTokenizer\n",
        "  # # Tokenize sentence\n",
        "  # tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # Remove stopwords (decrease accuracy)\n",
        "  # from nltk.corpus import stopwords\n",
        "  # stopwords_set = stopwords.words('english')\n",
        "  # tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  # X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  # remove numbers (decreases accuracy)\n",
        "  # import re\n",
        "  # tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  # X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # # join to back the tweets into a phrase\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "CfBSuzHlLR9r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess for non bert\n",
        "if not is_bert_preprocessing_enabled:\n",
        "  # Lowercase sentence\n",
        "  tweets = list(map(lambda tweet : tweet.lower(), tweets))\n",
        "  X_test = list(map(lambda tweet : tweet.lower(), X_test))\n",
        "\n",
        "  from nltk.tokenize import RegexpTokenizer\n",
        "  # Tokenize sentence\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tweets = list(map(lambda tweet : tokenizer.tokenize(tweet), tweets))\n",
        "  X_test = list(map(lambda tweet : tokenizer.tokenize(tweet), X_test))\n",
        "\n",
        "  # remove hashtags\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w.startswith(\"#\")], X_test))\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "  # Remove stopwords\n",
        "  stopwords_set = stopwords.words('english')\n",
        "  tweets = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], tweets))\n",
        "  X_test = list(map(lambda tweet  : [w for w in tweet if not w in stopwords_set], X_test))\n",
        "\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  # Lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweets = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], tweets))\n",
        "  X_test = list(map(lambda tweet : [lemmatizer.lemmatize(w) for w in tweet], X_test))\n",
        "\n",
        "  # remove numbers\n",
        "  import re\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if re.match(\"([0-9])+\", w) == None], X_test))\n",
        "\n",
        "  # remove user, url and other commond words\n",
        "  forbidden_words = [\"url\", \"user\"]\n",
        "  tweets = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], tweets))\n",
        "  X_test = list(map(lambda tweet : [w for w in tweet if not w in forbidden_words], X_test))\n",
        "\n",
        "  # # remove duplicates from the training data\n",
        "  # tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n",
        "  # print(len(tweets))\n",
        "  # tweets_df = pd.DataFrame({'tweets':tweets, 'labels':labels}).drop_duplicates(subset=['tweets'], keep='last')\n",
        "  # tweets = tweets_df[\"tweets\"].to_numpy()\n",
        "  # labels = tweets_df[\"labels\"].to_numpy()\n",
        "  # print(len(tweets))\n",
        "\n",
        "  # join back the tweets into a phrase\n",
        "  X_test = list(map(lambda tweet : \" \".join(tweet), X_test))\n",
        "  tweets = list(map(lambda tweet : \" \".join(tweet), tweets))\n"
      ],
      "metadata": {
        "id": "NMpcCsG3TarA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(tweets[i])\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "for i in range(20):\n",
        "  print(X_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMoLGO9XT17_",
        "outputId": "f5a30c45-5c6d-476c-ed5b-b0758a0cf3d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
            "glad i dot have taks tomorrow ! ! #thankful #startho\n",
            "1-3 vs celtics in the regular season = were fucked if we play them in the playoffs\n",
            "<user> i could actually kill that girl i'm so sorry ! ! !\n",
            "<user> <user> <user> i find that very hard to believe im afraid\n",
            "wish i could be out all night tonight ! <user>\n",
            "<user> i got kicked out the wgm\n",
            "rt <user> <user> <user> yes she is ! u tell it ! my lips are closed okay\n",
            "why is she so perfect <url>\n",
            "<user> hi harry ! did u havea good time in aus ? i didnt get 2 see u maybe next year ! follow me back if u can , would bea dreamcome truex\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "introduction to programming with c + + ( 2nd edition this solid foundation in the basics of c + + programming will ... <url>\n",
            "<user> i'm white . #aw\n",
            "<user> dan i love and miss you ! don't be sad #wheresthegeneral\n",
            "so many wonderful building in dc but still miss you <user>\n",
            "<user> it's annoying because i secretly find it so good ...\n",
            "the post-boom in spanish american fiction ( suny series in latin american and iberian thought and culture what ... <url>\n",
            "layers of the heart ( paperback this journey was inspired by a recent robbery that took place in the united sta ... <url>\n",
            "layers of the heart ( paperback this journey was inspired by a recent robbery that took place in the united sta ... <url>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\n",
            "<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\n",
            "i cant stay away from bug thats my baby\n",
            "<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
            "whenever i fall asleep watching the tv , i always wake up with a headache\n",
            "<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\n",
            "its whatever . in a terrible mood ( (\n",
            "yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\n",
            "my friend <user> text me to check up on me last night .\n",
            "<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\n",
            "watch some of y'all dumb asses get lock up today #happy420\n",
            "obsessed with #phasell <user> you killed it ! ! ! best album ever love yew roycee ! ! : * rt me\n",
            "<user> robert de niro is not gay .. but with a name like lewy , i'd understand if you were hahahahha no . i am sherlock\n",
            "<user> canada have to do it in grade 12 . but since we don't have grade 12 here , we do it in 11th . it sucks\n",
            "<user> please say hi to denmark ! that would be amazing ! ( <user> live on <url>\n",
            "finally am home now\n",
            "3x3 custom picture frame / poster frame 1.2 \" wide complete gold frame ( 2380763 9gd this frame is manufactured i ... <url>\n",
            "s / o to my new followers . mention me for a followback boo\n",
            "<user> yep , looks like the best team will stay up . proper on form . see you in a bit\n",
            "nhl's bettman : suspension criticism ' gamesmanship ' ( the associated press new york ( ap ) nhl ... <url> #predators #nhl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[797])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ZvIbuQUY01",
        "outputId": "3522055d-ffa2-492a-f11d-2085d70cfaca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<user> you can eat it , i need to cut down on food i've eaten loads #fattybombom xxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the training data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, y_train = shuffle(tweets, labels, random_state=84)"
      ],
      "metadata": {
        "id": "4YqG3R8WT4bM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save X data to a file\n",
        "def save_X_data(file_name, X):\n",
        "  f = open(DATA_PATH.format(file_name), \"w\")\n",
        "  for element in X:\n",
        "      # f.write(\" \".join(element) + \"\\n\")\n",
        "      f.write(element + \"\\n\")\n",
        "  f.close()\n",
        "\n",
        "save_X_data(\"X_train_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_train_processed.txt\", X_train)\n",
        "save_X_data(\"X_test_processed_bert_full.txt\" if is_bert_preprocessing_enabled else \"X_test_processed.txt\", X_test)"
      ],
      "metadata": {
        "id": "CKAb04McUEh5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save labels to file\n",
        "f = open(DATA_PATH.format(\"y_train.txt\"), \"w\")\n",
        "for label in y_train:\n",
        "  f.write(str(label) + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1v4OYiq0Wvda"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGfqcvEHhRj"
      },
      "source": [
        "# INITIAL EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YGXIVtwF1C7",
        "outputId": "3a361d17-22f6-4c5a-81c8-92c7559ab6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our dataset there are 1250000 negative tweets\n",
            "In our dataset there are 1250000 positive tweets\n"
          ]
        }
      ],
      "source": [
        "# get number of positive and negative tweets\n",
        "print(f\"In our dataset there are {(labels == 0).sum()} negative tweets\")\n",
        "print(f\"In our dataset there are {(labels == 1).sum()} positive tweets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivQ_UyYXD1zi",
        "outputId": "31b3461b-eb13-4aa8-aa46-070e83781c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average length of tweets with a negative sentiment is: 84.0923984\n",
            "The average length of tweets with a positive sentiment is: 67.835752\n"
          ]
        }
      ],
      "source": [
        "# get the average length of positive and negative examples\n",
        "def get_average_length(target_label):\n",
        "  len_tweets = []\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      len_tweets.append(len(tweet))\n",
        "  \n",
        "  return np.array(len_tweets).mean()\n",
        "\n",
        "# NOTE: postive tweets seem to be longer on average than negative tweets.\n",
        "# TODO: check if this thing can be used at our advantange in training\n",
        "print(f\"The average length of tweets with a negative sentiment is: {get_average_length(0)}\")\n",
        "print(f\"The average length of tweets with a positive sentiment is: {get_average_length(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get most frequent words in positive and negative examples\n",
        "def count_words(target_label):\n",
        "  words_occurrences = {}\n",
        "  for tweet, label in zip(tweets, labels):\n",
        "    if label == target_label:\n",
        "      for word in tweet:\n",
        "        words_occurrences[word] = words_occurrences.get(word, 0) + 1\n",
        "\n",
        "  return words_occurrences\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tweets = list(map(lambda x : x.split(), tweets))\n",
        "# negative tweets\n",
        "print(\"NEGATIVE TWEETS:\")\n",
        "negative = count_words(0)\n",
        "print(dict(Counter(negative).most_common(100)))\n",
        "# print uniques words in the negative tweets\n",
        "print(len(negative))\n",
        "\n",
        "# positive tweets\n",
        "print(\"\\n\\n\\n POSITIVE TWEETS:\")\n",
        "positive = count_words(1)\n",
        "print(dict(Counter(positive).most_common(100)))\n",
        "# print uniques words in the positive tweets\n",
        "print(len(positive))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTDyENY06On1",
        "outputId": "9506e492-6713-45c6-8369-7674bea51696"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEGATIVE TWEETS:\n",
            "{'<user>': 578374, '(': 544909, 'i': 537656, 'the': 434097, '<url>': 427974, '...': 416296, ',': 402968, '.': 369627, '!': 368079, 'to': 356966, 'a': 256250, 'and': 242747, 'of': 220383, 'you': 200754, 'my': 198536, 'is': 179956, 'in': 168647, 'me': 159073, '-': 157971, '\"': 150617, 'for': 148608, '?': 140983, 'this': 139045, ':': 126285, 'it': 122849, 'with': 107030, 'frame': 102308, 'so': 101611, 'on': 92682, 'that': 89985, 'but': 85435, '/': 84705, \"i'm\": 84161, 'have': 82791, ')': 80707, 'be': 74410, 'not': 71805, 'just': 67678, 'was': 67275, 'at': 59312, 'rt': 56134, 'like': 54616, '..': 54583, 'no': 53958, 'are': 53117, 'all': 52957, 'your': 50981, 'now': 49264, 'when': 48786, 'up': 48658, 'get': 48639, 'go': 47679, \"don't\": 46953, '&': 44897, '2': 44398, 'one': 44285, 'paperback': 43646, 'do': 43283, 'want': 42218, 'from': 41854, 'miss': 41156, 'know': 40974, 'out': 38237, 'u': 38141, \"can't\": 37414, 'really': 36435, 'we': 36100, 'what': 36011, 'too': 35804, \"'\": 35475, 'why': 34513, 'love': 33134, 'black': 32349, 'can': 31978, 'pack': 31923, 'see': 31615, 'will': 31584, 'complete': 31269, 'how': 31240, 'please': 31174, 'back': 30772, \"it's\": 30701, 'an': 30679, 'if': 30277, 'by': 30152, 'its': 30081, 'lol': 29924, 'x': 29702, 'wish': 29533, 'day': 29238, '1': 29080, 'picture': 28895, 'time': 28743, 'about': 28380, 'got': 28281, 'im': 27944, 'going': 27765, 'as': 27468, 'need': 27453, '3': 27359}\n",
            "405152\n",
            "\n",
            "\n",
            "\n",
            " POSITIVE TWEETS:\n",
            "{'<user>': 1027186, '!': 633599, 'i': 456072, 'you': 400487, '.': 387394, 'to': 346757, ',': 342877, 'the': 327183, 'a': 266343, 'and': 213979, 'my': 202918, '?': 193768, 'me': 183461, 'for': 157626, 'it': 156357, 'is': 131228, 'in': 128538, 'of': 119388, '\"': 115315, 'on': 111564, 'that': 111440, ')': 104664, 'be': 104470, '...': 100470, 'so': 99310, '<url>': 98885, 'with': 94628, 'love': 90903, 'have': 90869, 'your': 88000, \"i'm\": 87482, 'rt': 86221, 'just': 85005, 'this': 84132, 'but': 72505, 'good': 72419, 'follow': 71150, 'are': 67920, 'lol': 66737, '..': 66032, 'like': 65727, 'thanks': 62018, 'all': 61729, 'if': 60439, 'u': 59593, 'get': 59317, 'can': 57314, 'at': 56972, 'we': 56159, 'was': 55984, 'know': 55064, '&': 53821, 'do': 52712, 'up': 51685, 'not': 51469, 'day': 50672, 'will': 50517, 'please': 50516, 'one': 50492, 'what': 48010, 'when': 46091, 'now': 44101, ':': 44072, 'out': 43454, 'haha': 43126, '-': 42595, '<3': 41821, 'see': 41621, 'back': 40785, 'go': 40310, 'too': 39903, \"it's\": 38224, \"don't\": 37776, \"'\": 36841, 'thank': 36668, 'x': 36568, '(': 36054, 'about': 36010, 'got': 35583, 'no': 35514, 'time': 35415, 'how': 35256, 'today': 33969, 'from': 33678, 'happy': 31629, 'its': 31453, 'think': 31374, 'well': 30741, 'want': 30157, 'he': 30034, 'im': 29988, 'her': 29248, 'there': 29027, 'going': 28909, \"i'll\": 28571, '*': 28104, 'make': 27922, 'then': 26995, 'really': 26963, 'some': 26950}\n",
            "297267\n"
          ]
        }
      ]
    }
  ]
}