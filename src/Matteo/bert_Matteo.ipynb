{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_Matteo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UFGe452DZ13u"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "UfprFglXdnNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rHCbxCcMedU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "0Pf2ezJ0ZoWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr0C3Q-9Zkdk",
        "outputId": "e751dba0-bf18-42e9-9e0f-c6892b13fecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and global variables\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Dataset/{}'\n",
        "MODEL_PATH = '/content/drive/MyDrive/Colab Notebooks/CIL/Models/{}'\n",
        "\n",
        "# if set to true the trainig of the classifier models will be performed, otherwise the \n",
        "# models will be loaded from a file (if present)\n",
        "is_train_enabled = True"
      ],
      "metadata": {
        "id": "yp3uZOIHZrog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Preprocessed Data"
      ],
      "metadata": {
        "id": "UFGe452DZ13u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_X_data(file_name):\n",
        "  tweets = []\n",
        "  with open(DATA_PATH.format(file_name), 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        tweets.append(line.rstrip().split())\n",
        "  \n",
        "  return np.array(tweets)\n",
        "\n",
        "X_train = load_X_data(\"X_train_processed.txt\")\n",
        "X_test = load_X_data(\"X_test_processed.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCv_q-NNZ5o1",
        "outputId": "f3424589-1ad9-4325-e27e-0e3c23ae932e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_y_data(file_name):\n",
        "  labels = []\n",
        "  with open(DATA_PATH.format(file_name), 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        labels.append(int(line.rstrip()))\n",
        "  \n",
        "  return np.array(labels)\n",
        "\n",
        "y_train = load_y_data(\"y_train.txt\")"
      ],
      "metadata": {
        "id": "1KBvk8kYZ-yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(X_train[i])\n",
        "  print(X_test[i])\n",
        "  print(y_train[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYkorfbNbvtt",
        "outputId": "41d7c736-c672-4381-9f42-7db591ef5afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yes', 'even', 'realize', 'really', 'wanted', 'respond', 'want', 'buy', 'candy', 'something']\n",
            "['sea', 'doo', 'pro', 'sea', 'scooter', 'sport', 'portable', 'sea', 'doo', 'seascootersave', 'air']\n",
            "1\n",
            "['bradly', 'james', 'lowrey', 'bestfriend', 'mean', 'alot', 'justthoughtidleteveryoneknow']\n",
            "['shuck', 'well', 'work', 'week', 'come', 'cheer', 'oh', 'put', 'battery', 'calculator']\n",
            "1\n",
            "['mckleinusa', 'ashburn', 'series', 'leather', 'laptop', 'case', 'brown', 'clean', 'front', 'flap', 'design', 'secure', 'key', 'l']\n",
            "['cant', 'stay', 'away', 'bug', 'thats', 'baby']\n",
            "0\n",
            "['next', 'time', 'ima', 'come', 'yo', 'class', 'nd', 'wake', 'wanted', 'come', 'get', 'hug', 'sleep']\n",
            "['lol', 'im', 'perfectly', 'fine', 'contagious', 'anymore', 'lmao']\n",
            "0\n",
            "['trivial', 'pursuit', 'junior', 'second', 'edition', 'second', 'edition', 'junior', 'legendary', 'trivial', 'pursuit', 'game']\n",
            "['whenever', 'fall', 'asleep', 'watching', 'tv']\n",
            "0\n",
            "['new', 'bbm', 'add', 'please', 'pin']\n",
            "['need', 'get', 'rid', 'thing', 'scare', 'lol', 'need', 'car', 'either', 'need', 'driver', 'ed']\n",
            "1\n",
            "['morning', 'still', 'baby', 'everything', 'calmed', 'last', 'night', 'thanks', 'message', 'though']\n",
            "['whatever', 'terrible', 'mood']\n",
            "1\n",
            "['custom', 'picture', 'frame', 'poster', 'frame', 'wide', 'complete', 'smooth', 'cherry', 'frame', 'frame', 'man']\n",
            "['yes', 'rt', 'thanks', 'jordan']\n",
            "0\n",
            "['yeah', 'made', 'cry', 'like', 'little', 'bitch', 'love', 'luke']\n",
            "['friend', 'text', 'check', 'last', 'night']\n",
            "0\n",
            "['london', 'fgs', 'please', 'come', 'north']\n",
            "['followback', 'please', 'ur', 'unitytour', 'come', 'europe', 'sweden']\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "ag8SOn__eTSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the model into training test and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=33)"
      ],
      "metadata": {
        "id": "C1KaIacOjO5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# go from list type to 'Dataset' type. This object is requiered to train the model\n",
        "df = pd.DataFrame(X_train, columns =['Phrase'])\n",
        "train = Dataset.from_pandas(df).add_column(name=\"Label\", column=y_train)\n",
        "\n",
        "df = pd.DataFrame(X_val, columns =['Phrase'])\n",
        "val = Dataset.from_pandas(df).add_column(name=\"Label\", column=y_val)\n",
        "\n",
        "df = pd.DataFrame(X_test, columns =['Phrase'])\n",
        "test = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "7Jtc_yDEeTC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform tokenizationn of the dataset for BERT models\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(sentence):\n",
        "    return tokenizer(sentence['Phrase'], padding=True, truncation=True, max_length=30)\n",
        "\n",
        "train = train.map(tokenize_function, batched=True)\n",
        "test = test.map(tokenize_function, batched=True)\n",
        "val = val.map(tokenize_function, batched=True)\n",
        "\n",
        "for i in range(5):\n",
        "  print(train[i])\n",
        "  print(test[i])\n",
        "  print(val[i])"
      ],
      "metadata": {
        "id": "w10o8NjKghpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# go from 'Dataset' type to tensorflow so that our dataset can be used for training in keras\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "tf_train_dataset = train.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"Label\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=64,\n",
        ")\n",
        "\n",
        "tf_val_dataset = val.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"Label\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=64,\n",
        ")\n",
        "\n",
        "tf_test_dataset = test.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=64,\n",
        ")\n"
      ],
      "metadata": {
        "id": "nGJNp9hKgskD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "# Compute some variables needed to speed up training\n",
        "train_steps_per_epoch = int(len(tf_train_dataset) * (100/100) / 64)\n",
        "dev_steps_per_epoch = int(len(tf_val_dataset) * (100/100) / 64)\n",
        "\n",
        "\n",
        "# download pre-trained model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")"
      ],
      "metadata": {
        "id": "7MIruQBCgzBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_train_enabled:\n",
        "  model.fit(tf_train_dataset,\n",
        "            validation_data=tf_val_dataset,\n",
        "            epochs=20,\n",
        "            verbose=2,\n",
        "            steps_per_epoch=train_steps_per_epoch,\n",
        "            validation_steps=dev_steps_per_epoch,)\n",
        "  \n",
        "  model.save_pretrained(MODEL_PATH.format(\"bert_1\"))"
      ],
      "metadata": {
        "id": "8YCNaSE2j350"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load trained model from file\n",
        "model.load_weights(MODEL_PATH.format(\"bert_1/tf_model.h5\"))"
      ],
      "metadata": {
        "id": "CW0u49Gtj_Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(tf_test_dataset)\n",
        "y_pred = np.argmax(np.array(y_pred[0]), axis=-1)\n",
        "\n",
        "\n",
        "def write_output(file_name, Y):\n",
        "  f  = open(file_name, \"w\")\n",
        "  f.write(\"id,y\\n\")\n",
        "  id = 0\n",
        "  for y in Y:\n",
        "    f.write(str(id) + \",\" + str(y) + \"\\n\")\n",
        "    id = id + 1\n",
        "  f.close()\n",
        "\n",
        "write_output(DATA.PATH.format(\"bert_1_predictions.txt\"), y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "HANr8pnekavU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}